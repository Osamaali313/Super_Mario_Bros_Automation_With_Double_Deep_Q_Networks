{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNvfTjabuQ9pgXnXxeWlpxH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWA6OC5eZOwv","executionInfo":{"status":"ok","timestamp":1720461465978,"user_tz":-300,"elapsed":6686,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"eda621f2-34a2-430d-ca21-7009e63be2ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n","Installing collected packages: opendatasets\n","Successfully installed opendatasets-0.1.22\n"]}],"source":["!pip install opendatasets"]},{"cell_type":"code","source":["import opendatasets as od\n","\n","od.download(\"https://www.kaggle.com/datasets/alincijov/gym-super-mario-bros\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RItPAk-0Zaqq","executionInfo":{"status":"ok","timestamp":1720461566111,"user_tz":-300,"elapsed":12773,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"0c38acaf-9a2a-415d-987b-2e94bece6f6b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username: syedosamaalishah092\n","Your Kaggle Key: ··········\n","Dataset URL: https://www.kaggle.com/datasets/alincijov/gym-super-mario-bros\n","Downloading gym-super-mario-bros.zip to ./gym-super-mario-bros\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 502k/502k [00:00<00:00, 657kB/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torchvision import transforms as T\n","\n","import time, datetime\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","from pathlib import Path\n","from collections import deque\n","import random, datetime, os, copy\n","\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","\n","# change directory\n","import os\n","\n","os.chdir('/content/gym-super-mario-bros')\n","\n","# install controller\n","!pip --disable-pip-version-check install -q nes_py\n","from nes_py.wrappers import JoypadSpace\n","\n","import gym_super_mario_bros"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nm3E2X2Zl6m","executionInfo":{"status":"ok","timestamp":1720461590501,"user_tz":-300,"elapsed":20783,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"b751bddc-c31b-4e27-f805-8011518b9d3e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for nes_py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Initialize Super Mario environment\n","env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n","\n","# Limit the action-space to\n","#   0. walk right\n","#   1. jump right\n","env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n","\n","env.reset()\n","next_state, reward, done, info = env.step(action=0)\n","print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azi1W-rvaNLj","executionInfo":{"status":"ok","timestamp":1720461590502,"user_tz":-300,"elapsed":5,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"2f812677-7175-4f3e-a27d-d9bd32a252a9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n","  logger.warn(\n"]},{"output_type":"stream","name":"stdout","text":["(240, 256, 3),\n"," 0.0,\n"," False,\n"," {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(done, (bool, np.bool8)):\n"]}]},{"cell_type":"code","source":["class SkipFrame(gym.Wrapper):\n","    '''\n","        Custom wrapper that inherits from gy.Wrapper and implements the step() function.\n","        Use it to return only every skip nth frame\n","    '''\n","    def __init__(self, env, skip):\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = False\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info"],"metadata":{"id":"FoE84LD4aOUf","executionInfo":{"status":"ok","timestamp":1720461590502,"user_tz":-300,"elapsed":4,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class GrayScaleObservation(gym.ObservationWrapper):\n","    '''\n","        Common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information.\n","        Now the size of each state: [1, 240, 256]\n","    '''\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","        self.transform = T.Grayscale()\n","\n","    def permute_orientation(self, observation):\n","        observation = np.transpose(observation, (2, 0, 1))\n","        return torch.tensor(observation.copy(), dtype=torch.float)\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        return self.transform(observation)"],"metadata":{"id":"Lc8z-WDJaUT8","executionInfo":{"status":"ok","timestamp":1720461592776,"user_tz":-300,"elapsed":1447,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class ResizeObservation(gym.ObservationWrapper):\n","    '''\n","        Downsamples each observation into a square image.\n","        New size: [1, 84, 84]\n","    '''\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int): self.shape = (shape, shape)\n","        else: self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n","        return transforms(observation).squeeze(0)"],"metadata":{"id":"DPzosUkuacXc","executionInfo":{"status":"ok","timestamp":1720461594399,"user_tz":-300,"elapsed":5,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# preprocess environment\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStack(env, num_stack=4)"],"metadata":{"id":"iuXefJKhaiN5","executionInfo":{"status":"ok","timestamp":1720461597826,"user_tz":-300,"elapsed":5,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class MarioNet(nn.Module):\n","    \"\"\"\n","        input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, w, h = input_dim\n","\n","        if h != 84: raise ValueError(f\"Expecting input height: 84, got: {h}\")\n","        if w != 84: raise ValueError(f\"Expecting input width: 84, got: {w}\")\n","\n","        self.online = nn.Sequential(\n","            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim)\n","        )\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","        # freeze Q-target parameters\n","        for p in self.target.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, input, model):\n","        if model == \"online\":\n","            return self.online(input)\n","        elif model == \"target\":\n","            return self.target(input)"],"metadata":{"id":"_oH8tMBEak_H","executionInfo":{"status":"ok","timestamp":1720461598646,"user_tz":-300,"elapsed":13,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class Mario:\n","    '''\n","        Mario randomly explores with a chance of self.exploration_rate.\n","        When he chooses to exploit, he relies on MarioNet to provide the most optimal action.\n","    '''\n","    def __init__(self, state_dim, action_dim, use_cuda):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.use_cuda = use_cuda\n","        self.memory = deque(maxlen=100000)\n","        self.batch_size = 32\n","\n","        self.net = MarioNet(self.state_dim, self.action_dim).float()\n","        if self.use_cuda:\n","            self.net = self.net.to(device=\"cuda\")\n","\n","        self.exploration_rate = 1\n","        self.exploration_rate_decay = 0.99999975\n","        self.exploration_rate_min = 0.1\n","        self.curr_step = 0\n","\n","        self.save_every = 5e5\n","\n","\n","    def act(self, state):\n","        '''\n","            Given a state, choose an epsilon-greedy action and update value of step.\n","        '''\n","        # exploration\n","        if np.random.rand() < self.exploration_rate:\n","            action_idx = np.random.randint(self.action_dim)\n","\n","        # exploitation\n","\n","        else:\n","            state = state.__array__()\n","\n","            if self.use_cuda: state = torch.tensor(state).cuda()\n","            else: state = torch.tensor(state)\n","\n","            state = state.unsqueeze(0)\n","            action_values = self.net(state, model=\"online\")\n","            action_idx = torch.argmax(action_values, axis=1).item()\n","\n","        # decrease exploration_rate\n","        self.exploration_rate *= self.exploration_rate_decay\n","        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","        # increment step\n","        self.curr_step += 1\n","        return action_idx\n","\n","\n","    def cache(self, state, next_state, action, reward, done):\n","        \"\"\"\n","            Store the experience to self.memory (replay buffer).\n","        \"\"\"\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","\n","        state = torch.tensor(state)\n","        next_state = torch.tensor(next_state)\n","        action = torch.tensor([action])\n","        reward = torch.tensor([reward])\n","        done = torch.tensor([done])\n","\n","        if self.use_cuda:\n","            state = state.cuda()\n","            next_state = next_state.cuda()\n","            action = action.cuda()\n","            reward = reward.cuda()\n","            done = done.cuda()\n","\n","        self.memory.append((state, next_state, action, reward, done,))\n","\n","\n","    def recall(self):\n","        \"\"\"\n","            Retrieve a batch of experiences from memory\n","        \"\"\"\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"],"metadata":{"id":"so7Y6Vm8a1V1","executionInfo":{"status":"ok","timestamp":1720461603039,"user_tz":-300,"elapsed":12,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, use_cuda):\n","        super().__init__(state_dim, action_dim, use_cuda)\n","        self.gamma = 0.9\n","        self.burnin = 1e4  # min. experiences before training\n","        self.learn_every = 3  # no. of experiences between updates to Q_online\n","        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n","        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n","        self.loss_fn = torch.nn.SmoothL1Loss()\n","\n","\n","    def td_estimate(self, state, action):\n","        current_Q = self.net(state, model=\"online\")[\n","            np.arange(0, self.batch_size), action\n","        ]\n","        return current_Q\n","\n","    @torch.no_grad()\n","    def td_target(self, reward, next_state, done):\n","        next_state_Q = self.net(next_state, model=\"online\")\n","        best_action = torch.argmax(next_state_Q, axis=1)\n","        next_Q = self.net(next_state, model=\"target\")[\n","            np.arange(0, self.batch_size), best_action\n","        ]\n","        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n","\n","\n","    def update_Q_online(self, td_estimate, td_target):\n","        loss = self.loss_fn(td_estimate, td_target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n","\n","    def sync_Q_target(self):\n","        self.net.target.load_state_dict(self.net.online.state_dict())\n","\n","    def learn(self):\n","        if self.curr_step % self.sync_every == 0: self.sync_Q_target()\n","        if self.curr_step % self.save_every == 0: self.save()\n","        if self.curr_step < self.burnin: return None, None\n","        if self.curr_step % self.learn_every != 0: return None, None\n","\n","        # Sample from memory\n","        state, next_state, action, reward, done = self.recall()\n","\n","        # Get TD Estimate\n","        td_est = self.td_estimate(state, action)\n","\n","        # Get TD Target\n","        td_tgt = self.td_target(reward, next_state, done)\n","\n","        # Backpropagate loss through Q_online\n","        loss = self.update_Q_online(td_est, td_tgt)\n","\n","        return (td_est.mean().item(), loss)"],"metadata":{"id":"VGmX2ktydcFn","executionInfo":{"status":"ok","timestamp":1720461608571,"user_tz":-300,"elapsed":3,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class MetricLogger:\n","    def __init__(self):\n","        # history metrics\n","        self.ep_rewards = []\n","        self.ep_lengths = []\n","        self.ep_avg_losses = []\n","        self.ep_avg_qs = []\n","\n","        # moving averages, added for every call to record()\n","        self.moving_avg_ep_rewards = []\n","        self.moving_avg_ep_lengths = []\n","        self.moving_avg_ep_avg_losses = []\n","        self.moving_avg_ep_avg_qs = []\n","\n","        # current episode metric\n","        self.init_episode()\n","\n","        # timing\n","        self.record_time = time.time()\n","\n","    def log_step(self, reward, loss, q):\n","        self.curr_ep_reward += reward\n","        self.curr_ep_length += 1\n","        if loss:\n","            self.curr_ep_loss += loss\n","            self.curr_ep_q += q\n","            self.curr_ep_loss_length += 1\n","\n","    def log_episode(self):\n","        self.ep_rewards.append(self.curr_ep_reward)\n","        self.ep_lengths.append(self.curr_ep_length)\n","        if self.curr_ep_loss_length == 0:\n","            ep_avg_loss = 0\n","            ep_avg_q = 0\n","        else:\n","            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n","            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n","        self.ep_avg_losses.append(ep_avg_loss)\n","        self.ep_avg_qs.append(ep_avg_q)\n","\n","        self.init_episode()\n","\n","    def init_episode(self):\n","        self.curr_ep_reward = 0.0\n","        self.curr_ep_length = 0\n","        self.curr_ep_loss = 0.0\n","        self.curr_ep_q = 0.0\n","        self.curr_ep_loss_length = 0\n","\n","    def record(self, episode, epsilon, step):\n","        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n","        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n","        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n","        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n","        self.moving_avg_ep_rewards.append(mean_ep_reward)\n","        self.moving_avg_ep_lengths.append(mean_ep_length)\n","        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n","        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n","\n","        last_record_time = self.record_time\n","        self.record_time = time.time()\n","        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n","\n","        print(\n","            \"Episode:{:4d}  :: Step:{:5d}  :: Epsilon:{:8.3f}  :: Mean_Reward:{:8.3f}  :: \" \\\n","            \"Mean_Length:{:8.3f}  :: Mean_Loss:{:4.3f}  :: Mean_Q_Value:{:8.3f}  :: \" \\\n","            \"Time_Delta:{:8.3f} \"\n","\n","            .format(episode, step, epsilon, mean_ep_reward, mean_ep_length,\n","                    mean_ep_loss, mean_ep_q, time_since_last_record)\n","        )"],"metadata":{"id":"4B_OYtP-d46A","executionInfo":{"status":"ok","timestamp":1720461611789,"user_tz":-300,"elapsed":5,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","print(f\"Using CUDA: {use_cuda}\")\n","print()\n","\n","mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, use_cuda=use_cuda)\n","\n","logger = MetricLogger()\n","\n","episodes = 301\n","for e in range(episodes):\n","\n","    state = env.reset()\n","\n","    # Play the game!\n","    while True:\n","\n","        # Run agent on the state\n","        action = mario.act(state)\n","\n","        # Agent performs action\n","        next_state, reward, done, info = env.step(action)\n","\n","        # Remember\n","        mario.cache(state, next_state, action, reward, done)\n","\n","        # Learn\n","        q, loss = mario.learn()\n","\n","        # Logging\n","        logger.log_step(reward, loss, q)\n","\n","        # Update state\n","        state = next_state\n","\n","        # Check if end of game\n","        if done or info[\"flag_get\"]:\n","            break\n","\n","    logger.log_episode()\n","\n","    if e % 25 == 0:\n","        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LoIcXASueIEt","executionInfo":{"status":"ok","timestamp":1720462523680,"user_tz":-300,"elapsed":906087,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"7fcef7f8-1539-4c7a-ac4e-e8d5438fb634"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: True\n","\n","Episode:   0  :: Step:   40  :: Epsilon:   1.000  :: Mean_Reward: 231.000  :: Mean_Length:  40.000  :: Mean_Loss:0.000  :: Mean_Q_Value:   0.000  :: Time_Delta:   0.535 \n","Episode:  25  :: Step: 6145  :: Epsilon:   0.998  :: Mean_Reward: 584.962  :: Mean_Length: 236.346  :: Mean_Loss:0.000  :: Mean_Q_Value:   0.000  :: Time_Delta:  71.034 \n","Episode:  50  :: Step:14425  :: Epsilon:   0.996  :: Mean_Reward: 687.843  :: Mean_Length: 282.843  :: Mean_Loss:0.197  :: Mean_Q_Value:   0.495  :: Time_Delta: 103.017 \n","Episode:  75  :: Step:20937  :: Epsilon:   0.995  :: Mean_Reward: 707.211  :: Mean_Length: 275.487  :: Mean_Loss:0.261  :: Mean_Q_Value:   1.283  :: Time_Delta:  83.861 \n","Episode: 100  :: Step:26008  :: Epsilon:   0.994  :: Mean_Reward: 720.660  :: Mean_Length: 259.680  :: Mean_Loss:0.309  :: Mean_Q_Value:   2.284  :: Time_Delta:  65.572 \n","Episode: 125  :: Step:32259  :: Epsilon:   0.992  :: Mean_Reward: 748.420  :: Mean_Length: 261.140  :: Mean_Loss:0.412  :: Mean_Q_Value:   3.791  :: Time_Delta:  80.755 \n","Episode: 150  :: Step:38479  :: Epsilon:   0.990  :: Mean_Reward: 716.510  :: Mean_Length: 240.540  :: Mean_Loss:0.426  :: Mean_Q_Value:   5.462  :: Time_Delta:  80.286 \n","Episode: 175  :: Step:42908  :: Epsilon:   0.989  :: Mean_Reward: 682.190  :: Mean_Length: 219.710  :: Mean_Loss:0.453  :: Mean_Q_Value:   6.952  :: Time_Delta:  56.870 \n","Episode: 200  :: Step:49183  :: Epsilon:   0.988  :: Mean_Reward: 648.130  :: Mean_Length: 231.750  :: Mean_Loss:0.471  :: Mean_Q_Value:   8.049  :: Time_Delta:  81.253 \n","Episode: 225  :: Step:53035  :: Epsilon:   0.987  :: Mean_Reward: 609.020  :: Mean_Length: 207.760  :: Mean_Loss:0.532  :: Mean_Q_Value:   9.350  :: Time_Delta:  49.938 \n","Episode: 250  :: Step:61033  :: Epsilon:   0.985  :: Mean_Reward: 618.150  :: Mean_Length: 225.540  :: Mean_Loss:0.569  :: Mean_Q_Value:  10.371  :: Time_Delta: 103.041 \n","Episode: 275  :: Step:67052  :: Epsilon:   0.983  :: Mean_Reward: 639.160  :: Mean_Length: 241.440  :: Mean_Loss:0.610  :: Mean_Q_Value:  11.420  :: Time_Delta:  77.374 \n","Episode: 300  :: Step:71054  :: Epsilon:   0.982  :: Mean_Reward: 637.350  :: Mean_Length: 218.710  :: Mean_Loss:0.655  :: Mean_Q_Value:  12.389  :: Time_Delta:  51.742 \n"]}]},{"cell_type":"code","source":["!pip --disable-pip-version-check install -q pyvirtualdisplay\n","\n","from pyvirtualdisplay import Display\n","\n","from IPython import display as ipythondisplay\n","from IPython.display import HTML\n","\n","from gym.wrappers import Monitor\n","from glob import glob\n","\n","import base64\n","import io\n","\n","display = Display(visible=0, size=(600, 300))\n","display.start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"94ttNLn5eYUl","executionInfo":{"status":"error","timestamp":1720462528967,"user_tz":-300,"elapsed":5298,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}},"outputId":"2b1df157-8327-41e5-b295-5927faa3dd55"},"execution_count":16,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'Monitor' from 'gym.wrappers' (/usr/local/lib/python3.10/dist-packages/gym/wrappers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-c2cd6f82d090>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Monitor' from 'gym.wrappers' (/usr/local/lib/python3.10/dist-packages/gym/wrappers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","    mp4list = glob('/contant/working/video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","                 </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","\n","def wrap_env(env):\n","    env = Monitor(env, '/content/working/video', force=True)\n","    return env\n","\n","env = wrap_env(env)"],"metadata":{"id":"YvOd3cy1edBs","executionInfo":{"status":"aborted","timestamp":1720462528967,"user_tz":-300,"elapsed":6,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = env.reset()\n","\n","# Play the game!\n","while True:\n","\n","    # Run agent on the state\n","    action = mario.act(state)\n","\n","    # Agent performs action\n","    next_state, reward, done, info = env.step(action)\n","\n","    # Update state\n","    state = next_state\n","\n","    # Check if end of game\n","    if done or info[\"flag_get\"]:\n","        break\n","\n","env.close()\n","show_video()"],"metadata":{"id":"-iL4TWKZexrt","executionInfo":{"status":"aborted","timestamp":1720461483771,"user_tz":-300,"elapsed":10,"user":{"displayName":"Syed Ali","userId":"00026085019319269903"}}},"execution_count":null,"outputs":[]}]}